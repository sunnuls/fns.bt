#!/usr/bin/env python3
"""
System check script for FanslyMotion v2.0
Verifies all optimizations and requirements are properly configured.
"""
import sys
import os

def print_header(text):
    """Print formatted header."""
    print(f"\n{'='*60}")
    print(f" {text}")
    print(f"{'='*60}\n")

def check_python():
    """Check Python version."""
    print("üêç Python Version:")
    version = sys.version_info
    print(f"   Version: {version.major}.{version.minor}.{version.micro}")
    
    if version.major == 3 and version.minor >= 10:
        print("   ‚úÖ Python version OK")
        return True
    else:
        print("   ‚ùå Python 3.10+ required")
        return False

def check_cuda():
    """Check CUDA availability."""
    print("\nüéÆ CUDA Status:")
    try:
        import torch
        cuda_available = torch.cuda.is_available()
        
        if cuda_available:
            print(f"   ‚úÖ CUDA available: {torch.version.cuda}")
            print(f"   GPU: {torch.cuda.get_device_name(0)}")
            print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
            
            # Check compute capability
            capability = torch.cuda.get_device_capability()
            print(f"   Compute capability: {capability[0]}.{capability[1]}")
            
            if capability[0] >= 8:
                print(f"   ‚úÖ Ampere or newer - TF32 available")
            else:
                print(f"   ‚ÑπÔ∏è  Pre-Ampere - TF32 not available")
            
            return True
        else:
            print("   ‚ùå CUDA not available")
            return False
    except ImportError:
        print("   ‚ùå PyTorch not installed")
        return False

def check_packages():
    """Check required packages."""
    print("\nüì¶ Required Packages:")
    
    packages = {
        'torch': '2.1+',
        'torchvision': '0.16+',
        'diffusers': '0.25+',
        'transformers': '4.36+',
        'xformers': '0.0.23',
        'fastapi': '0.109+',
        'aiogram': '3.3+',
        'redis': '5.0+',
        'rq': '1.16+',
        'imageio': '2.33+',
        'opencv-python': '4.9+',
        'Pillow': '10.2+',
    }
    
    all_ok = True
    
    for package, version in packages.items():
        try:
            if package == 'opencv-python':
                module = __import__('cv2')
            elif package == 'Pillow':
                module = __import__('PIL')
            else:
                module = __import__(package)
            
            pkg_version = getattr(module, '__version__', 'unknown')
            
            if package == 'xformers':
                # xformers is critical
                print(f"   ‚úÖ {package}: {pkg_version} (CRITICAL for performance)")
            else:
                print(f"   ‚úÖ {package}: {pkg_version}")
                
        except ImportError:
            if package == 'xformers':
                print(f"   ‚ö†Ô∏è  {package}: NOT INSTALLED (CRITICAL - install with: pip install xformers==0.0.23)")
                print(f"       Without xformers, generation will be 2x slower!")
            else:
                print(f"   ‚ùå {package}: NOT INSTALLED (required: {version})")
            all_ok = False
    
    return all_ok

def check_optimizations():
    """Check if optimizations are enabled."""
    print("\n‚ö° Optimizations:")
    
    try:
        import torch
        
        # Check TF32
        if hasattr(torch.backends.cuda, 'matmul'):
            tf32_enabled = torch.backends.cuda.matmul.allow_tf32
            if tf32_enabled:
                print(f"   ‚úÖ TF32 enabled for matmul")
            else:
                print(f"   ‚ÑπÔ∏è  TF32 not enabled (auto-enabled for Ampere+ GPUs)")
        
        # Check xformers
        try:
            import xformers
            print(f"   ‚úÖ xformers available: {xformers.__version__}")
        except ImportError:
            print(f"   ‚ùå xformers NOT available - Performance will be degraded!")
            print(f"      Install with: pip install xformers==0.0.23")
        
        return True
    except ImportError:
        print("   ‚ùå Cannot check optimizations - PyTorch not available")
        return False

def check_config():
    """Check configuration files."""
    print("\n‚öôÔ∏è  Configuration:")
    
    # Check .env
    if os.path.exists('.env'):
        print("   ‚úÖ .env file found")
        
        # Check BOT_TOKEN
        with open('.env', 'r') as f:
            env_content = f.read()
            if 'BOT_TOKEN=' in env_content:
                if 'your_telegram_bot_token' not in env_content:
                    print("   ‚úÖ BOT_TOKEN configured")
                else:
                    print("   ‚ö†Ô∏è  BOT_TOKEN not set (using placeholder)")
            else:
                print("   ‚ùå BOT_TOKEN not found in .env")
    else:
        print("   ‚ùå .env file not found")
        print("      Create from env.example: copy env.example .env")
    
    # Check directories
    dirs = ['storage/hot', 'storage/archive', 'cache/models', 'cache/torch']
    for dir_path in dirs:
        if os.path.exists(dir_path):
            print(f"   ‚úÖ {dir_path} exists")
        else:
            print(f"   ‚ÑπÔ∏è  {dir_path} not found (will be created on startup)")

def check_redis():
    """Check Redis connection."""
    print("\nüî¥ Redis:")
    
    try:
        import redis
        
        # Try to connect
        r = redis.Redis(host='localhost', port=6379, db=0, socket_connect_timeout=2)
        r.ping()
        print("   ‚úÖ Redis is running and accessible")
        
        # Check queue
        from rq import Queue
        queue = Queue(connection=r)
        print(f"   ‚úÖ RQ Queue accessible, jobs in queue: {len(queue)}")
        
        return True
    except Exception as e:
        print(f"   ‚ùå Redis not accessible: {e}")
        print(f"      Start Redis with: redis-server")
        return False

def check_model():
    """Check if model is downloaded."""
    print("\nü§ñ Model Status:")
    
    cache_dirs = [
        'cache/models/models--stabilityai--stable-video-diffusion-img2vid-xt',
        'cache/models/stable-video-diffusion-img2vid-xt'
    ]
    
    model_found = False
    for cache_dir in cache_dirs:
        if os.path.exists(cache_dir):
            print(f"   ‚úÖ Model found in: {cache_dir}")
            
            # Check size
            total_size = 0
            for dirpath, dirnames, filenames in os.walk(cache_dir):
                for filename in filenames:
                    filepath = os.path.join(dirpath, filename)
                    total_size += os.path.getsize(filepath)
            
            size_gb = total_size / (1024**3)
            print(f"   Model size: {size_gb:.2f} GB")
            
            if size_gb > 5:
                print(f"   ‚úÖ Model appears to be complete")
            else:
                print(f"   ‚ö†Ô∏è  Model size seems small, might be incomplete")
            
            model_found = True
            break
    
    if not model_found:
        print("   ‚ÑπÔ∏è  Model not found in cache")
        print("      Will be downloaded on first use (~7GB)")
    
    return True

def check_gpu_memory():
    """Check available GPU memory."""
    print("\nüíæ GPU Memory:")
    
    try:
        import torch
        
        if torch.cuda.is_available():
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            reserved = torch.cuda.memory_reserved(0) / 1024**3
            free = total_memory - allocated
            
            print(f"   Total VRAM: {total_memory:.1f} GB")
            print(f"   Allocated: {allocated:.1f} GB")
            print(f"   Reserved: {reserved:.1f} GB")
            print(f"   Free: {free:.1f} GB")
            
            # Recommendations based on VRAM
            if total_memory >= 20:
                print(f"   ‚úÖ Sufficient for 1080p generation")
            elif total_memory >= 16:
                print(f"   ‚úÖ Recommended for 720p generation")
                print(f"   ‚ö†Ô∏è  1080p might be tight")
            elif total_memory >= 12:
                print(f"   ‚ö†Ô∏è  Recommended to use 720p or lower")
            else:
                print(f"   ‚ö†Ô∏è  Low VRAM - use 480p or 360p")
            
            return True
        else:
            print("   ‚ùå No GPU available")
            return False
    except ImportError:
        print("   ‚ùå Cannot check GPU memory - PyTorch not available")
        return False

def print_summary(results):
    """Print summary of checks."""
    print_header("Summary")
    
    total = len(results)
    passed = sum(results.values())
    
    print(f"Checks passed: {passed}/{total}\n")
    
    for check, result in results.items():
        status = "‚úÖ" if result else "‚ùå"
        print(f"  {status} {check}")
    
    print()
    
    if passed == total:
        print("üéâ All checks passed! System is ready.")
        print("\nüìö Next steps:")
        print("   1. Make sure BOT_TOKEN is set in .env")
        print("   2. Start Redis: redis-server")
        print("   3. Run: python -m bot.bot")
        return 0
    else:
        print("‚ö†Ô∏è  Some checks failed. Please fix the issues above.")
        print("\nüìñ Documentation:")
        print("   - UPDATE_GUIDE.md - Update instructions")
        print("   - TROUBLESHOOTING_RU.md - Common problems")
        print("   - PERFORMANCE_TIPS.md - Optimization guide")
        return 1

def main():
    """Main check function."""
    print_header("FanslyMotion v2.0 - System Check")
    
    results = {}
    
    results['Python Version'] = check_python()
    results['CUDA'] = check_cuda()
    results['Packages'] = check_packages()
    results['Optimizations'] = check_optimizations()
    results['Configuration'] = check_config()
    results['Redis'] = check_redis()
    results['Model'] = check_model()
    results['GPU Memory'] = check_gpu_memory()
    
    exit_code = print_summary(results)
    sys.exit(exit_code)

if __name__ == '__main__':
    main()

